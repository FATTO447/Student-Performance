# -*- coding: utf-8 -*-
"""StudentPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RC-dQtxHjDqQ56uDuE9qqMJ35IEoJG0F
"""

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files
uploaded = files.upload()

import pandas as pd
df = pd.read_csv('student_score.csv')

import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd
df = pd.read_csv('student_score.csv')
df.info()

df.describe()

import pandas as pd
df = pd.read_csv('student_score.csv')
df.head()

print(df.shape)

import pandas as pd
df = pd.read_csv('student_score.csv')
df.sum()

df.nunique()

import pandas as pd
df = pd.read_csv('student_score.csv')
df.describe()

import pandas as pd
df = pd.read_csv('student_score.csv')
df_c = df.duplicated()
print(df_c)
print(df[df_c])

import pandas as pd
df = pd.read_csv('student_score.csv')
df_c = df.duplicated()
df_d = df.drop_duplicates()
print(df_d)
print(df[df_c])

diff = set(df["Hours"]).difference(set(df["Scores"]))
print(diff)

df.isnull().sum()

print(df.columns.tolist())

df = pd.read_csv('student_score.csv')
df ['Hours'] = pd.to_numeric(df['Hours'] , errors = 'coerce')
df['Scores'] = pd.to_numeric(df['Scores'] , errors = 'coerce')

df = pd.read_csv('student_score.csv')
print(df['Hours'].value_counts(normalize = True))
print(df['Scores'].value_counts(normalize = True ))

num_col = ['Hours' , 'Scores']

inconsists_pass = df[(df['Scores'] > 100) | (df['Scores'] < 0)]
consist = df[(df['Scores'] <= 100) | (df['Scores'] >= 0 )]

print(inconsists_pass)
print(consist)

import missingno as msno
msno.matrix(df)
plt.show()

df = pd.read_csv('student_score.csv')
df.hist(figsize = (10 , 6))
plt.show()
sns.countplot( x = 'Hours' , data = df, color='black')
plt.show()
sns.countplot( x = 'Scores' , data = df, color='gray' )
plt.show()

df = pd.read_csv('student_score.csv')
df.hist(figsize = (10,8))
plt.show()
df.boxplot( column = 'Hours' , by = 'Scores' , figsize = (10,8) , color = 'black')
plt.show()

df = pd.read_csv('student_score.csv')
plt.style.use('ggplot')
for col in num_col:
    plt.figure(figsize=(10,6))
    sns.histplot(df[col] , kde = True , bins = 30 , color = 'black')
    plt.title(f"Distrubtion {col}" , fontweight='bold')
    plt.xlabel(col , fontweight='bold' , color = 'black')
    plt.ylabel("Frequency" , fontweight='bold' , color = 'black')
    plt.show()

df = pd.read_csv('student_score.csv')
plt.style.use('dark_background')
plt.style.use('ggplot')
plt.scatter( x = 'Hours' , y = 'Scores' , data = df , color = 'black')
plt.xlabel('Hours', fontweight='bold' ,color = 'black')
plt.ylabel('Scores', fontweight='bold' , color='black')
plt.show()

df = pd.read_csv('student_score.csv')
plt.style.use('grayscale')
plt.figure(figsize = (10,8))
sns.scatterplot(data = df , x="Hours", y="Scores" , color = 'black')
plt.xlabel("Hours" , fontweight='bold' , color = 'black')
plt.ylabel("Scores" , fontweight='bold' , color = 'black')
plt.show()

df = pd.read_csv('student_score.csv')
plt.figure(figsize = (10,8))
sns.boxplot(data = df[num_col])
plt.show()

plt.figure(figsize = (10 , 8))

sns.pairplot(df )
plt.show()

plt.figure(figsize = (10,8))
plt.style.use('dark_background')
plt.style.use('bmh')
for col in num_col:
  plt.hist(df[col] , bins = 10 , color = 'gray')
  plt.xlabel(col , fontweight = 'bold' , color = 'black')
  plt.ylabel('frequency' , fontweight = 'bold' , color = 'gray')
  plt.title(f"Distrubtions {col}")
  plt.show()

fig , ax = plt.subplots()
plt.style.use('bmh')
for col in num_col :
  ax.plot(df[col] , label = col)
ax.grid(True , linestyle = '--')
plt.xlabel("index" , fontweight = 'bold' , color = 'red')
plt.ylabel("values" , fontweight = "bold" , color = 'red')
ax.tick_params(labelcolor='r', labelsize='medium', width=3)
plt.title("Line plots of numerical columns")
plt.legend()
plt.show()

plt.figure(figsize = (10,8))
sns.violinplot(data = df[num_col])
plt.show()

plt.figure(figsize = (10 , 8))
sns.relplot(x = 'Hours' , y = 'Scores' , data = df , hue = 'Scores')
plt.show()

sns.heatmap(df.corr() , annot = True , cmap = 'magma' , linecolor = 'black')
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, roc_auc_score
)

X = df[['Hours']]
df['Pass'] = (df['Scores'] >= 50).astype(int)
y = df['Pass']

# Split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# Model
model = LogisticRegression()
model.fit(x_train, y_train)

# Predict
y_pred = model.predict(x_test)

# Summary
summary = {
    "Accuracy": accuracy_score(y_test, y_pred),
    "Precision": precision_score(y_test, y_pred),
    "Recall": recall_score(y_test, y_pred),
    "F1 Score": f1_score(y_test, y_pred),
    "Classification Report": classification_report(y_test, y_pred)
}
summary_df = pd.DataFrame([summary])
print(summary_df)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, cmap='magma')
plt.title("Confusion Matrix")
plt.show()

# ROC Curve
fpr, tpr, threshold = roc_curve(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)
plt.style.use('ggplot')
plt.figure(figsize=(10, 8))
plt.plot(fpr, tpr, color='black', label='AUC = %.2f' % auc)
plt.title('ROC Curve Regerission', fontweight='bold', color='black')
plt.xlabel('False Positive Rate', fontweight='bold', color='black')
plt.ylabel('True Positive Rate', fontweight='bold', color='black')
plt.legend()
plt.show()

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, roc_auc_score
)

X = df[['Hours']]
df['Pass'] = (df['Scores'] >= 50).astype(int)
y = df['Pass']

# Split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)

# Scale
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# Model
model = DecisionTreeClassifier()
model.fit(x_train, y_train)

# Predict
y_pred = model.predict(x_test)

# Summary
summary = {
    "Accuracy": accuracy_score(y_test, y_pred),
    "Precision": precision_score(y_test, y_pred),
    "Recall": recall_score(y_test, y_pred),
    "F1 Score": f1_score(y_test, y_pred),
    "Classification Report": classification_report(y_test, y_pred)
}
summary_df = pd.DataFrame([summary])
print(summary_df)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, cmap='magma')
plt.title("Confusion Matrix")
plt.show()

# ROC Curve
fpr, tpr, threshold = roc_curve(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

plt.style.use('dark_background')
plt.figure(figsize=(10, 8))
plt.plot(fpr, tpr, color='red', label='AUC = %.2f' % auc)
plt.title('ROC Curve Decision Tree', fontweight='bold', color='red')
plt.xlabel('False Positive Rate', fontweight='bold', color='red')
plt.ylabel('True Positive Rate', fontweight='bold', color='red')
plt.legend()
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, roc_auc_score
)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

#split
x = df[['Hours']]
y = df ['pass'] # Corrected 'Pass' to 'pass'

x_train , x_test , y_train , y_test = train_test_split(x , y , test_size = 0.2 , random_state= 42)


scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

model = RandomForestClassifier()
model.fit(x_train, y_train)

y_pred = model.predict(x_test)

summary = {
     "Accuracy": accuracy_score(y_test, y_pred),
    "Precision": precision_score(y_test, y_pred),
    "Recall": recall_score(y_test, y_pred),
    "F1 Score": f1_score(y_test, y_pred),
    "Classification Report": classification_report(y_test, y_pred)
}
summary_df = pd.DataFrame([summary])
print(summary_df)

cm = confusion_matrix(y_test , y_pred)
sns.heatmap(cm , annot = True , cmap = 'magma')
plt.title("Confusion Matrix" , fontweight = 'bold')
plt.show()

fpr , tpr , threshold = roc_curve(y_test , model.predict_proba(x_test)[:, 1])
auc = roc_auc_score(y_test , y_pred)

plt.style.use('dark_background')
plt.figure(figsize= (10,8))
plt.plot(fpr , tpr , color = 'red' , label = 'AUC = %.2F' %auc)
plt.plot([0,1] , [0,1] , linestyle = '--' , color = 'red')
plt.title('ROC Curve Random  ', fontweight='bold', color='red')
plt.xlabel('False Positive Rate', fontweight='bold', color='red')
plt.ylabel('True Positive Rate', fontweight='bold', color='red')
plt.legend()
plt.show()

from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, roc_auc_score
)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

#split
x = df[['Hours']]
df['pass'] = (df['Scores'] >=50).astype(int)
y = df ['Pass']

x_train , x_test , y_train , y_test = train_test_split(x , y , test_size = 0.3 , random_state= 10)


scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# Model
model = SVC(probability=True) # Set probability to True
model.fit(x_train, y_train)

# Predict
y_pred = model.predict(x_test)

# Summary
summary = {
     "Accuracy": accuracy_score(y_test, y_pred),
    "Precision": precision_score(y_test, y_pred),
    "Recall": recall_score(y_test, y_pred),
    "F1 Score": f1_score(y_test, y_pred),
    "Classification Report": classification_report(y_test, y_pred)
}
summary_df = pd.DataFrame([summary])
print(summary_df)

cm = confusion_matrix(y_test , y_pred)
sns.heatmap(cm , annot = True , cmap = 'magma')
plt.title("Confusion Matrix" , fontweight = 'bold')
plt.show()

# ROC Curve
fpr , tpr , threshold = roc_curve(y_test , model.predict_proba(x_test)[:, 1])
auc = roc_auc_score(y_test , y_pred)

plt.style.use('dark_background')
plt.figure(figsize= (10,8))
plt.plot(fpr , tpr , color = 'red' , label = 'AUC = %.2F' %auc)
plt.plot([0,1] , [0,1] , linestyle = '--' , color = 'red')
plt.title('ROC Curve', fontweight='bold', color='red')
plt.xlabel('False Positive Rate', fontweight='bold', color='red')
plt.ylabel('True Positive Rate', fontweight='bold', color='red')
plt.legend()
plt.show()

"""Supervised : KNeighborsClassifier"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, roc_auc_score
)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

#split
x = df[['Hours']]
df['pass'] = (df['Scores'] >=50).astype(int)
y = df ['Pass']

x_train , x_test , y_train , y_test = train_test_split(x , y , test_size = 0.2 , random_state= 42)


scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# Model
model = KNeighborsClassifier()
model.fit(x_train, y_train)

# Predict
y_pred = model.predict(x_test)

# Summary
summary = {
     "Accuracy": accuracy_score(y_test, y_pred),
    "Precision": precision_score(y_test, y_pred),
    "Recall": recall_score(y_test, y_pred),
    "F1 Score": f1_score(y_test, y_pred),
    "Classification Report": classification_report(y_test, y_pred)
}
summary_df = pd.DataFrame([summary])
print(summary_df)

cm = confusion_matrix(y_test , y_pred)
sns.heatmap(cm , annot = True , cmap = 'magma')
plt.title("Confusion Matrix" , fontweight = 'bold')
plt.show()

# ROC Curve
fpr , tpr , threshold = roc_curve(y_test , model.predict_proba(x_test)[:, 1])
auc = roc_auc_score(y_test , y_pred)

plt.style.use('dark_background')
plt.figure(figsize= (10,8))
plt.plot(fpr , tpr , color = 'red' , label = 'AUC = %.2F' %auc)
plt.plot([0,1] , [0,1] , linestyle = '--' , color = 'red')
plt.title('ROC Curve', fontweight='bold', color='red')
plt.xlabel('False Positive Rate', fontweight='bold', color='red')
plt.ylabel('True Positive Rate', fontweight='bold', color='red')
plt.legend()
plt.show()

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, roc_auc_score
)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

#split
x = df[['Hours']]
df['pass'] = (df['Scores'] >=50).astype(int)
y = df ['pass']

x_train , x_test , y_train , y_test = train_test_split(x , y , test_size = 0.3 , random_state= 10)


scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# Model
model = GaussianNB()
model.fit(x_train, y_train)

# Predict
y_pred = model.predict(x_test)

# Summary
summary = {
     "Accuracy": accuracy_score(y_test, y_pred),
    "Precision": precision_score(y_test, y_pred),
    "Recall": recall_score(y_test, y_pred),
    "F1 Score": f1_score(y_test, y_pred),
    "Classification Report": classification_report(y_test, y_pred)
}
summary_df = pd.DataFrame([summary])
print(summary_df)

cm = confusion_matrix(y_test , y_pred)
sns.heatmap(cm , annot = True , cmap = 'magma')
plt.title("Confusion Matrix" , fontweight = 'bold')
plt.show()

# ROC Curve
fpr , tpr , threshold = roc_curve(y_test , model.predict_proba(x_test)[:, 1])
auc = roc_auc_score(y_test , y_pred)

plt.style.use('dark_background')
plt.figure(figsize= (10,8))
plt.plot(fpr , tpr , color = 'red' , label = 'AUC = %.2F' %auc)
plt.plot([0,1] , [0,1] , linestyle = '--' , color = 'red')
plt.title('ROC Curve', fontweight='bold', color='red')
plt.xlabel('False Positive Rate', fontweight='bold', color='red')
plt.ylabel('True Positive Rate', fontweight='bold', color='red')
plt.legend()
plt.show()

"""Since our dataset does not contain a real target column, creating one (e.g., Pass/Fail) would lead to overfitting because the model will only learn the artificial rule we imposed. Therefore, unsupervised methods (like KMeans or Hierarchical clustering) are more suitable, as they allow us to discover natural groupings in the data without assuming a predefined target.The reported accuracy in supervised models is not reliable, since it only reflects the artificially created target column. This leads to overfitting and unrealistic performance. Therefore, accuracy here is misleading and cannot be used as a valid measure."""

from sklearn.cluster import KMeans , AgglomerativeClustering
from sklearn.metrics import silhouette_score, accuracy_score, confusion_matrix
import numpy as np
import pandas as pd # Import pandas

# Reuse the x and y variables from previous cells
x = df[['Hours', 'Scores']]
y = (df['Scores'] >=50).astype(int)

kmeans = KMeans(n_clusters = 2 , random_state = 42 , n_init = 10) # Corrected random_size to random_state
clusters_kmeans = kmeans.fit_predict(x)
silhouette_kmeans = silhouette_score(x, clusters_kmeans) # Corrected silohuette_kmeans

mapping_kmeans = {}

for c in np.unique(clusters_kmeans) :
  label_in_clusters = y[clusters_kmeans == c]
  if not label_in_clusters.empty:
    mapping_kmeans[c] = label_in_clusters.mode()[0]
  else :
    mapping_kmeans[c] = 0

y_pred_kmeans = np.array([mapping_kmeans[c] for c in clusters_kmeans]) # Corrected for in syntax
accuracy_kmeans = accuracy_score(y , y_pred_kmeans) # Corrected accuarcy_score

hier = AgglomerativeClustering(n_clusters = 2)
clusters_hier = hier.fit_predict(x)
silhouette_hier = silhouette_score(x , clusters_hier) # Corrected silhoutte_hier and cluster_hier

mapping_hier = {}
for h in np.unique(clusters_hier):
  label_in_clusters = y[clusters_hier == h]
  if not label_in_clusters.empty :
    mapping_hier[h] = label_in_clusters.mode()[0]
  else :
    mapping_hier[h] = 0 # Corrected == 0 to = 0

y_pred_hier = np.array([mapping_hier[c] for c in clusters_hier]) # Corrected for in syntax and y_pred_hier - to y_pred_hier =
accuracy_hier = accuracy_score(y , y_pred_hier) # Corrected accuarcy_hier

results = pd.DataFrame({
    "Model" : ["KMeans", "Hierarchical"],
    "Silhouette Score" : [silhouette_kmeans , silhouette_hier] , # Corrected silohuette_kmeans
    "Accuracy" : [accuracy_kmeans , accuracy_hier] # Corrected Accuarcy
})
print(results)

cm_kmeans = confusion_matrix(y , y_pred_kmeans)
sns.heatmap(cm_kmeans , annot = True , cmap = 'magma')
plt.title("Confusion Matrix Kmeans ")
plt.show()

cm_hier = confusion_matrix(y , y_pred_hier)
sns.heatmap(cm_hier , annot = True , cmap = 'magma')
plt.title("Confusion Matrix Hier ")
plt.show()
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

axes[0].scatter(x['Hours'], x['Scores'], c=clusters_kmeans, cmap='coolwarm')
axes[0].set_title("KMeans Clusters")

axes[1].scatter(x['Hours'], x['Scores'], c=clusters_hier, cmap='viridis')
axes[1].set_title("Hierarchical Clusters")

plt.show()

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.model_selection import train_test_split

x_train , x_test , y_train , y_test = train_test_split(x , y , test_size = 0.2 , random_state = 42)

kmeans = KMeans(n_clusters = 2 , random_state = 42 , n_init = 10)
kmeans.fit(x_train)

train_labels = kmeans.labels_
train_silhouette = silhouette_score(x_train , train_labels)

test_labels = kmeans.predict(x_test) # Corrected to use x_test
test_silhouette = silhouette_score(x_test , test_labels)

results = pd.DataFrame({
    "Model" : ["Kmeans"] ,
    "Train Data Silhouette " : [train_silhouette] ,
    "Test Data Silhouette " : [test_silhouette]
})
print(results)

plt.figure(figsize= (10,8))
plt.scatter(x_train['Hours'] , x_train['Scores'] , c = train_labels , cmap = 'coolwarm')
plt.title("Train Data Clusters")
plt.show()

plt.figure(figsize= (10,8))
plt.scatter(x_test['Hours'] , x_test['Scores'] , c = test_labels , cmap = 'coolwarm')
plt.title("Test Data Clusters")
plt.show()

"""ðŸ“Œ Project Report
1. Objective

The goal of this project was to analyze the relationship between study hours and scores to explore whether supervised or unsupervised learning methods are more appropriate.

2. Data Description

Features:

Hours: Number of study hours.

Scores: Student exam scores.

Note: No target column (e.g., Pass/Fail), which limits supervised learning.

3. Approach
Supervised Models

Tried Logistic Regression by creating a target (Pass/Fail).

The results initially showed very high accuracy (100%). However, this was misleading because the target variable was artificially engineered, which caused the model to overfit the training data. To address this, I experimented with different random states and trainâ€“test splits, but the fundamental issue remained: the absence of a natural target variable makes supervised learning unreliable in this context.


Unsupervised Models

KMeans Clustering and Hierarchical Clustering were applied.

Evaluation done using Silhouette Score.

KMeans Silhouette Score: 0.682 (Accuracy ~1.0)

Hierarchical Silhouette Score: 0.615 (Accuracy ~0.88)

Train/Test Silhouette validation confirmed KMeans was more stable:

Train Silhouette Score: 0.656

Test Silhouette Score: 0.752

4. Findings

Supervised models are not reliable due to the absence of a real target column.

Unsupervised models (especially KMeans) provide a more realistic clustering of students based on study hours and scores.

Accuracy from unsupervised learning should not be interpreted the same way as in supervised models; instead, Silhouette Score gives a more meaningful evaluation.

5. Conclusion

Unsupervised learning was the best choice for this dataset.

KMeans clustering produced the most consistent and meaningful results.

Future work: If a real target column (e.g., Pass/Fail) becomes available, supervised models can be revisited.
"""